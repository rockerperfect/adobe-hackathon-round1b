# -----------------------------------------------------------------------------
# Dockerfile - Adobe Hackathon Round 1B: Persona-Driven Document Intelligence
#
# Purpose:
#     Containerizes the persona-driven intelligence pipeline for reproducible,
#     portable execution across environments. Supports multilingual PDF parsing,
#     NLP processing, and vegetarian filtering for menu planning use cases.
#
# Structure:
#     - Multi-stage build for optimized image size and caching
#     - Stage 1: Pre-downloads ML models and language data (build-time optimization)
#     - Stage 2: Application runtime with system dependencies and code
#     - Externalized configuration via environment variables and volume mounts
#
# Dependencies:
#     - requirements.txt: All Python dependencies with pinned versions
#     - System packages: Tesseract OCR with multilingual support, build tools
#     - ML Models: sentence-transformers, NLTK data (cached in build stage)
#
# Integration Points:
#     - Input/output via volume mounts to /app/input and /app/output
#     - Configuration via environment variables (see config/settings.py)
#     - Logging to stdout/stderr (structured JSON format)
#
# Security & Best Practices:
#     - No secrets or sensitive data hardcoded
#     - Non-root user execution
#     - Minimal attack surface with slim base images
#     - Layer caching optimization for faster rebuilds
#
# Usage:
#     docker build -t adobe-hackathon-pipeline .
#     docker run --rm -v "$(pwd)/Collection 1:/app/input" \
#                      -v "$(pwd)/output:/app/output" \
#                      -e LOG_LEVEL=INFO \
#                      adobe-hackathon-pipeline
# -----------------------------------------------------------------------------

# Stage 1: Model downloading and preparation
FROM python:3.10-slim AS model-stage

# Set build-time environment variables
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1

# Install minimal dependencies for model downloading
RUN pip install --no-cache-dir sentence-transformers>=2.2.0 transformers>=4.35.0 torch>=2.0.0 nltk>=3.8.0

# Pre-download NLTK data only (sentence transformers will be downloaded at runtime)
RUN python -c "import nltk; nltk.download('punkt'); nltk.download('punkt_tab'); nltk.download('stopwords')"

# Stage 2: Application runtime environment
FROM python:3.10-slim

# Set runtime environment variables (externalized configuration)
ENV PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    DEBIAN_FRONTEND=noninteractive

# Install system dependencies for multilingual PDF processing and OCR
RUN apt-get update && apt-get install -y --no-install-recommends \
    # Core build tools
    build-essential \
    # PDF processing
    poppler-utils \
    # OCR engine and multilingual language packs
    tesseract-ocr \
    tesseract-ocr-jpn tesseract-ocr-chi-sim tesseract-ocr-chi-tra \
    tesseract-ocr-ara tesseract-ocr-hin tesseract-ocr-kor \
    tesseract-ocr-tha tesseract-ocr-rus tesseract-ocr-fra \
    tesseract-ocr-deu tesseract-ocr-spa tesseract-ocr-ita \
    # Additional utilities
    curl \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/* \
    && rm -rf /tmp/* /var/tmp/*

# Create non-root user for security
RUN groupadd -r appuser && useradd -r -g appuser -d /app -s /bin/bash appuser

# Copy pre-downloaded NLTK data from build stage
COPY --from=model-stage /root/nltk_data /home/appuser/
RUN chown -R appuser:appuser /home/appuser/

# Set application working directory
WORKDIR /app

# Copy Python dependencies file for caching optimization
COPY requirements.txt .

# Install Python dependencies with security and performance optimizations
RUN pip install --no-cache-dir --upgrade pip setuptools wheel && \
    pip install --no-cache-dir -r requirements.txt

# Copy application source code (excluding unnecessary files via .dockerignore)
COPY . .

# Create necessary directories with proper permissions
RUN mkdir -p /app/input /app/output /app/logs /app/config/cache \
    && chown -R appuser:appuser /app \
    && chmod -R 755 /app

# Switch to non-root user for security
USER appuser

# Set environment variables for optimal performance
ENV PYTHONPATH=/app
ENV PYTHONUNBUFFERED=1
ENV NLTK_DATA=/home/appuser/nltk_data

# PRE-WARM MODELS during build to eliminate 55s startup time
# This is the KEY OPTIMIZATION for 20-30s processing times
RUN echo "ðŸ”¥ Pre-warming models for optimal performance..." && \
    python pre_warm_models_simple.py && \
    echo "âœ… Models pre-warmed successfully - processing should take 20-30s per collection!"

# Verify model installation and dependencies
RUN python -c "import sentence_transformers, nltk, fitz, PIL; print('Core dependencies verified')"

# Expose application port (if applicable - not currently used)
# EXPOSE 8000

# Health check for container monitoring
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python -c "import sys; sys.exit(0)" || exit 1

# Default command - run main application with externalized configuration
CMD ["python", "main.py"]

# -----------------------------------------------------------------------------
# Build and run instructions:
# docker build -t adobe-hackathon-pipeline .
# docker run -v $(pwd)/Collection\ 1:/app/Collection\ 1 \
#            -v $(pwd)/Collection\ 2:/app/Collection\ 2 \
#            -v $(pwd)/Collection\ 3:/app/Collection\ 3 \
#            -v $(pwd)/output:/app/output \
#            -e ENVIRONMENT=production \
#            adobe-hackathon-pipeline
# -----------------------------------------------------------------------------
